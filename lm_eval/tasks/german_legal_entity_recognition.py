"""
German Legal Entity Recognition Dataset
https://link.springer.com/chapter/10.1007/978-3-030-33220-4_20

Git: https://github.com/elenanereiss/Legal-Entity-Recognition

The dataset consists of a selection of German court decisions from 2017 and 2018 which have been published online by the Federal Ministry of Justice and Consumer Protection. 
The documents come from seven federal courts: Federal Labour Court (BAG), Federal Fiscal Court (BFH), Federal Court of Justice (BGH), 
Federal Patent Court (BPatG), Federal Social Court (BSG), Federal Constitutional Court (BVerfG) and Federal Administrative Court (BVerwG). Data can be used for 
Named Entity Recognition (NER) in German language documents from the legal domain. For this purpose the documents were manually annotated with 19 more fine-grained  and 
7 rather general semantic classes. The dataset consists of approximately 67,000 sentences and contains 54,000 annotated entities.
"""
import datasets
from lm_eval.base import Task, rf
from lm_eval.metrics import mean
from functools import partial
import numpy as np

_CITATION = """
@inproceedings{leitner2019fine,
  author = {Elena Leitner and Georg Rehm and Julian Moreno-Schneider},
  title = {{Fine-grained Named Entity Recognition in Legal Documents}},
  booktitle = {Semantic Systems. The Power of AI and Knowledge
                  Graphs. Proceedings of the 15th International Conference
                  (SEMANTiCS 2019)},
  year = 2019,
  editor = {Maribel Acosta and Philippe CudrÃ©-Mauroux and Maria
                  Maleshkova and Tassilo Pellegrini and Harald Sack and York
                  Sure-Vetter},
  keywords = {aip},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  number = {11702},
  address = {Karlsruhe, Germany},
  month = 9,
  note = {10/11 September 2019},
  pages = {272--287},
  pdf = {https://link.springer.com/content/pdf/10.1007%2F978-3-030-33220-4_20.pdf}}
}"""

# Helper functions for aggregation (separate function for each metric)
def _german_ler_agg_precision(key, items):
    references, predictions = zip(*items)
    precision_metric = datasets.load_metric("precision")
    return precision_metric.compute(references=references, predictions=predictions, average='macro', labels= np.unique(predictions))[key]

def _german_ler_agg_recall(key, items):
    references, predictions = zip(*items)
    recall_metric = datasets.load_metric("recall")
    return recall_metric.compute(references=references, predictions=predictions, average='macro', labels= np.unique(predictions))[key]

def _german_ler_agg_f1(key, items):
    references, predictions = zip(*items)
    f1_metric = datasets.load_metric("f1")
    return f1_metric.compute(references=references, predictions=predictions, average='macro', labels= np.unique(predictions))[key]

"""class GermanLegalEntityRecognition(Task):
    VERSION = 0
    DATASET_PATH = 'german_legal_entity_recognition'
    DATASET_NAME = 'all'

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return False
      
    def training_docs(self):
        if self.has_training_docs():
            if self._training_docs is None:
                self._training_docs = list(self.dataset["train"])
            
            return self._training_docs

    def validation_docs(self):
        pass

    def test_docs(self):
        pass
      
    def doc_to_text(self, doc): 
      return "tokens: "+ doc['tokens'] + ""+ "NER tag: "

    def doc_to_target(self, doc):
        # The prepended `" "` is required to space out the `doc_to_text` and
        # `doc_to_target` strings.
        target = doc['ner_tags']

        return " " + str(target)

    def construct_requests(self, doc, ctx):
        Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or
            test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        
        
        b_person = rf.loglikelihood(ctx, " "+"B-PER")
        b_judge = rf.loglikelihood(ctx, " "+"B-RR")
        b_laywer = rf.loglikelihood(ctx, " "+"B-AN")
        b_country = rf.loglikelihood(ctx, " "+"B-LD")
        b_city = rf.loglikelihood(ctx, " "+"B-ST")
        b_street = rf.loglikelihood(ctx, " "+"B-STR")
        b_landscape = rf.loglikelihood(ctx, " "+"B-LDS")
        b_organization = rf.loglikelihood(ctx, " "+"B-ORG")
        b_company = rf.loglikelihood(ctx, " "+"B-UN")
        b_institution = rf.loglikelihood(ctx, " "+"B-INN")
        b_court = rf.loglikelihood(ctx, " "+"B-GRT")
        b_brand = rf.loglikelihood(ctx, " "+"B-MRK")
        b_law = rf.loglikelihood(ctx, " "+"B-GS")
        b_ordinance = rf.loglikelihood(ctx, " "+"B-VO")
        b_eu_legal_norm = rf.loglikelihood(ctx, " "+"B-EUN")
        b_regulation = rf.loglikelihood(ctx, " "+"B-VS")
        b_contract = rf.loglikelihood(ctx, " "+"B-VT")
        b_court_decision = rf.loglikelihood(ctx, " "+"B-RS")
        b_legal_literature = rf.loglikelihood(ctx, " "+"B-LIT")

        i_person = rf.loglikelihood(ctx, " "+"I-PER")
        i_judge = rf.loglikelihood(ctx, " "+"I-RR")
        i_laywer = rf.loglikelihood(ctx, " "+"I-AN")
        i_country = rf.loglikelihood(ctx, " "+"I-LD")
        i_city = rf.loglikelihood(ctx, " "+"I-ST")
        i_street = rf.loglikelihood(ctx, " "+"I-STR")
        i_landscape = rf.loglikelihood(ctx, " "+"I-LDS")
        i_organization = rf.loglikelihood(ctx, " "+"I-ORG")
        i_company = rf.loglikelihood(ctx, " "+"I-UN")
        i_institution = rf.loglikelihood(ctx, " "+"I-INN")
        i_court = rf.loglikelihood(ctx, " "+"I-GRT")
        i_brand = rf.loglikelihood(ctx, " "+"I-MRK")
        i_law = rf.loglikelihood(ctx, " "+"I-GS")
        i_ordinance = rf.loglikelihood(ctx, " "+"I-VO")
        i_eu_legal_norm = rf.loglikelihood(ctx, " "+"I-EUN")
        i_regulation = rf.loglikelihood(ctx, " "+"I-VS")
        i_contract = rf.loglikelihood(ctx, " "+"I-VT")
        i_court_decision = rf.loglikelihood(ctx, " "+"I-RS")
        i_legal_literature = rf.loglikelihood(ctx, " "+"I-LIT")

        other = rf.loglikelihood(ctx, " "+"O")

        return b_person, b_judge, b_laywer, b_country, b_city, b_street, b_landscape, b_organization, b_company, b_institution, b_court, b_brand, b_law, b_ordinance, b_eu_legal_norm, b_regulation, b_contract, b_court_decision, b_legal_literature, i_person, i_judge, i_laywer, i_country, i_city, i_street, i_landscape, i_organization, i_company, i_institution, i_court, i_brand, i_law, i_ordinance, i_eu_legal_norm, i_regulation, i_contract, i_court_decision, i_legal_literature, other

    def process_results(self, doc, results):
        Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        
        b_person, b_judge, b_laywer, b_country, b_city, b_street, b_landscape, b_organization, b_company, b_institution, b_court, b_brand, b_law, b_ordinance, b_eu_legal_norm, b_regulation, b_contract, b_court_decision, b_legal_literature, i_person, i_judge, i_laywer, i_country, i_city, i_street, i_landscape, i_organization, i_company, i_institution, i_court, i_brand, i_law, i_ordinance, i_eu_legal_norm, i_regulation, i_contract, i_court_decision, i_legal_literature, other = results
        
        pred = float('-inf')
        
        for i in results:
          if i[0] > pred:
            pred = results.index(i)
                      
        true_label = doc['ner_tags']
        
        return {"acc": pred==true_label, "precision":(true_label, pred), "recall":(true_label, pred), "f1":(true_label, pred)}

    def aggregation(self):
        
        :returns: {str: [metric_score] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metric scores
        
        return {"acc":mean, "precision": partial(_german_ler_agg_precision, "precision"), 
                "recall" : partial(_german_ler_agg_recall, "recall"), 
                "f1" : partial(_german_ler_agg_f1, "f1")}

    def higher_is_better(self):
        return {"acc":True, "precision":True, "recall":True, "f1":True}"""
class GermanLegalEntityRecognition(Task):
    VERSION = 0
    DATASET_PATH = 'jfrenz/legalglue/german_ler'
    DATASET_NAME = None#'german_ler'

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return False
      
    def training_docs(self):
        if self.has_training_docs():
            if self._training_docs is None:
                self._training_docs = list(self.dataset["train"])
            
            return self._training_docs

    def validation_docs(self):
        pass

    def test_docs(self):
        pass
      
    def doc_to_text(self, doc): 
      return "tokens: "+ ' '.join(doc['tokens']) + "\n\n"+ "NER tags: "

    def doc_to_target(self, doc):
        # The prepended `" "` is required to space out the `doc_to_text` and
        # `doc_to_target` strings.
        target = doc['ner_tags']

        return " " + str(target)

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or
            test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        

        ner_tag_sequence = rf.greedy_until(ctx, [38])

        while len(ner_tag_sequence) < len(ctx.split(' ')):
            tmp = rf.greedy_until(ctx[len(ner_tag_sequence):], [38])
            ner_tag_sequence += tmp
        print(ner_tag_sequence)
        return ner_tag_sequence, rf.loglikelihood(ctx, [' '])

    def process_results(self, doc, results):
        """Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        """
        tag_sequence = results
        print(tag_sequence)
        true_label = doc['ner_tags']
        
        return {"acc": pred==true_label, "precision":(true_label, pred), "recall":(true_label, pred), "f1":(true_label, pred)}

    def aggregation(self):
        """
        :returns: {str: [metric_score] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metric scores
        """
        return {"acc":mean, "precision": partial(_german_ler_agg_precision, "precision"), 
                "recall" : partial(_german_ler_agg_recall, "recall"), 
                "f1" : partial(_german_ler_agg_f1, "f1")}

    def higher_is_better(self):
        return {"acc":True, "precision":True, "recall":True, "f1":True}    
